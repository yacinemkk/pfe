{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ IoT Device Identification - Data Exploration & Preprocessing\n",
    "\n",
    "Ce notebook explore et pr√©traite le dataset IPFIX pour l'identification des appareils IoT.\n",
    "\n",
    "## üìã Objectifs\n",
    "1. Charger et explorer les donn√©es brutes\n",
    "2. Analyser la distribution des classes\n",
    "3. Nettoyer et normaliser les features\n",
    "4. Cr√©er des s√©quences temporelles\n",
    "5. Sauvegarder les donn√©es trait√©es sur Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup - Montage Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "PROJECT_DIR = '/content/drive/MyDrive/PFE_IoT'\n",
    "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
    "os.makedirs(f'{PROJECT_DIR}/data/processed', exist_ok=True)\n",
    "os.makedirs(f'{PROJECT_DIR}/results', exist_ok=True)\n",
    "print(f\"‚úÖ Projet configur√© dans: {PROJECT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation des d√©pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas numpy scikit-learn tqdm matplotlib seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "print(\"‚úÖ D√©pendances install√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Configuration des chemins\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT**: Uploadez vos fichiers CSV (home1.csv, home2.csv, etc.) dans le dossier sp√©cifi√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚¨áÔ∏è MODIFIEZ CE CHEMIN selon l'emplacement de vos donn√©es\n",
    "RAW_DATA_DIR = f'{PROJECT_DIR}/data/raw/IPFIX ML Instances'\n",
    "PROCESSED_DATA_DIR = f'{PROJECT_DIR}/data/processed'\n",
    "RESULTS_DIR = f'{PROJECT_DIR}/results'\n",
    "\n",
    "# V√©rification\n",
    "if os.path.exists(RAW_DATA_DIR):\n",
    "    files = [f for f in os.listdir(RAW_DATA_DIR) if f.endswith('.csv')]\n",
    "    print(f\"‚úÖ Trouv√© {len(files)} fichiers CSV\")\n",
    "    for f in sorted(files):\n",
    "        size_mb = os.path.getsize(os.path.join(RAW_DATA_DIR, f)) / (1024*1024)\n",
    "        print(f\"   - {f}: {size_mb:.1f} MB\")\n",
    "else:\n",
    "    print(f\"‚ùå Dossier non trouv√©: {RAW_DATA_DIR}\")\n",
    "    print(\"Cr√©ez le dossier et uploadez les fichiers CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Exploration des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger un fichier pour exploration\n",
    "sample_file = f'{RAW_DATA_DIR}/home1.csv'\n",
    "\n",
    "if os.path.exists(sample_file):\n",
    "    print(\"Chargement de home1.csv (√©chantillon)...\")\n",
    "    df_sample = pd.read_csv(sample_file, nrows=10000)\n",
    "    print(f\"\\nüìê Dimensions: {df_sample.shape}\")\n",
    "    print(f\"\\nüìã Colonnes ({len(df_sample.columns)}):\")\n",
    "    print(df_sample.columns.tolist())\n",
    "    print(f\"\\nüìä Types:\")\n",
    "    print(df_sample.dtypes)\n",
    "    print(f\"\\nüîç Aper√ßu:\")\n",
    "    display(df_sample.head())\n",
    "else:\n",
    "    print(\"Fichier sample non trouv√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des appareils\n",
    "if 'df_sample' in locals():\n",
    "    device_counts = df_sample['device'].value_counts()\n",
    "    print(\"\\nüì± Distribution des appareils (√©chantillon):\")\n",
    "    print(device_counts)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    device_counts.plot(kind='bar')\n",
    "    plt.title('Distribution des appareils IoT')\n",
    "    plt.xlabel('Appareil')\n",
    "    plt.ylabel('Nombre de flux')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_DIR}/device_distribution_sample.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Pr√©traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du pr√©traitement\n",
    "FEATURES_TO_KEEP = [\n",
    "    'duration', 'ipProto',\n",
    "    'outPacketCount', 'outByteCount', 'inPacketCount', 'inByteCount',\n",
    "    'outSmallPktCount', 'outLargePktCount', 'outNonEmptyPktCount', 'outDataByteCount',\n",
    "    'outAvgIAT', 'outFirstNonEmptyPktSize', 'outMaxPktSize', 'outStdevPayloadSize',\n",
    "    'outStdevIAT', 'outAvgPacketSize',\n",
    "    'inSmallPktCount', 'inLargePktCount', 'inNonEmptyPktCount', 'inDataByteCount',\n",
    "    'inAvgIAT', 'inFirstNonEmptyPktSize', 'inMaxPktSize', 'inStdevPayloadSize',\n",
    "    'inStdevIAT', 'inAvgPacketSize',\n",
    "    'http', 'https', 'smb', 'dns', 'ntp', 'tcp', 'udp', 'ssdp', 'lan', 'wan',\n",
    "    'deviceInitiated'\n",
    "]\n",
    "\n",
    "FEATURES_TO_DROP = ['start', 'srcMac', 'destMac', 'srcIP', 'destIP', 'srcPort', 'destPort']\n",
    "LABEL_COLUMN = 'device'\n",
    "SEQUENCE_LENGTH = 10\n",
    "STRIDE = 5\n",
    "MIN_SAMPLES_PER_CLASS = 500\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.1\n",
    "\n",
    "print(\"‚úÖ Configuration charg√©e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data(data_dir, max_files=None):\n",
    "    \"\"\"Charge tous les fichiers CSV\"\"\"\n",
    "    dfs = []\n",
    "    csv_files = sorted([f for f in os.listdir(data_dir) if f.startswith('home') and f.endswith('.csv')])\n",
    "    \n",
    "    if max_files:\n",
    "        csv_files = csv_files[:max_files]\n",
    "    \n",
    "    for f in tqdm(csv_files, desc=\"Chargement\"):\n",
    "        df = pd.read_csv(os.path.join(data_dir, f))\n",
    "        df['source_file'] = f.replace('.csv', '')\n",
    "        dfs.append(df)\n",
    "        print(f\"  {f}: {len(df):,} lignes\")\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"Nettoie les donn√©es\"\"\"\n",
    "    print(f\"Avant nettoyage: {len(df):,} lignes\")\n",
    "    \n",
    "    # Supprimer doublons et NA\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.dropna(subset=[LABEL_COLUMN])\n",
    "    \n",
    "    # Supprimer colonnes non pertinentes\n",
    "    for col in FEATURES_TO_DROP:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(columns=col)\n",
    "    \n",
    "    # Filtrer classes rares\n",
    "    class_counts = df[LABEL_COLUMN].value_counts()\n",
    "    valid_classes = class_counts[class_counts >= MIN_SAMPLES_PER_CLASS].index\n",
    "    df = df[df[LABEL_COLUMN].isin(valid_classes)]\n",
    "    \n",
    "    print(f\"Apr√®s nettoyage: {len(df):,} lignes\")\n",
    "    print(f\"Classes conserv√©es: {len(valid_classes)}\")\n",
    "    \n",
    "    return df, valid_classes\n",
    "\n",
    "print(\"‚úÖ Fonctions d√©finies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö° Chargement des donn√©es (ajustez max_files pour tester)\n",
    "MAX_FILES = None  # Mettre un nombre (ex: 3) pour tester plus vite\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CHARGEMENT DES DONN√âES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df = load_all_data(RAW_DATA_DIR, max_files=MAX_FILES)\n",
    "print(f\"\\n‚úÖ Total: {len(df):,} flux\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NETTOYAGE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_clean, valid_classes = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution finale\n",
    "plt.figure(figsize=(14, 6))\n",
    "df_clean[LABEL_COLUMN].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution finale des classes d\\'appareils IoT')\n",
    "plt.xlabel('Appareil')\n",
    "plt.ylabel('Nombre de flux')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/device_distribution_final.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Distribution:\")\n",
    "print(df_clean[LABEL_COLUMN].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, source_groups, seq_length=SEQUENCE_LENGTH, stride=STRIDE):\n",
    "    \"\"\"Cr√©e des s√©quences temporelles\"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    for group_id in tqdm(np.unique(source_groups), desc=\"Cr√©ation s√©quences\"):\n",
    "        mask = source_groups == group_id\n",
    "        X_group = X[mask]\n",
    "        y_group = y[mask]\n",
    "        \n",
    "        n_samples = len(X_group) - seq_length\n",
    "        for i in range(0, max(1, n_samples), stride):\n",
    "            if i + seq_length <= len(X_group):\n",
    "                X_seq.append(X_group[i:i+seq_length])\n",
    "                y_seq.append(y_group[i+seq_length-1])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "print(\"‚úÖ Fonction de s√©quen√ßage d√©finie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©lection des features et encodage\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"S√âLECTION FEATURES & ENCODAGE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "features = [c for c in FEATURES_TO_KEEP if c in df_clean.columns]\n",
    "print(f\"Features s√©lectionn√©es: {len(features)}\")\n",
    "\n",
    "X = df_clean[features].values\n",
    "y = df_clean[LABEL_COLUMN].values\n",
    "source_groups = df_clean['source_file'].values\n",
    "\n",
    "# Encodage des labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"Classes: {num_classes}\")\n",
    "print(f\"Labels: {label_encoder.classes_[:5]}...\")\n",
    "\n",
    "# Normalisation\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "print(f\"Shape X: {X_normalized.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation des s√©quences\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CR√âATION DES S√âQUENCES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_seq, y_seq = create_sequences(X_normalized, y_encoded, source_groups)\n",
    "print(f\"\\n‚úÖ S√©quences cr√©√©es: {X_seq.shape}\")\n",
    "print(f\"   Input shape: (samples, timesteps, features) = {X_seq.shape}\")\n",
    "print(f\"   Labels shape: {y_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division Train/Val/Test\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIVISION TRAIN/VAL/TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_seq, y_seq, test_size=TEST_SIZE, random_state=42, stratify=y_seq\n",
    ")\n",
    "\n",
    "val_ratio = VAL_SIZE / (1 - TEST_SIZE)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=(1-val_ratio), random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train):,} ({len(X_train)/len(X_seq)*100:.1f}%)\")\n",
    "print(f\"Val:   {len(X_val):,} ({len(X_val)/len(X_seq)*100:.1f}%)\")\n",
    "print(f\"Test:  {len(X_test):,} ({len(X_test)/len(X_seq)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde sur Google Drive\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAUVEGARDE SUR GOOGLE DRIVE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.save(f'{PROCESSED_DATA_DIR}/X_train.npy', X_train)\n",
    "np.save(f'{PROCESSED_DATA_DIR}/X_val.npy', X_val)\n",
    "np.save(f'{PROCESSED_DATA_DIR}/X_test.npy', X_test)\n",
    "np.save(f'{PROCESSED_DATA_DIR}/y_train.npy', y_train)\n",
    "np.save(f'{PROCESSED_DATA_DIR}/y_val.npy', y_val)\n",
    "np.save(f'{PROCESSED_DATA_DIR}/y_test.npy', y_test)\n",
    "\n",
    "# Sauvegarder le pr√©processeur\n",
    "with open(f'{PROCESSED_DATA_DIR}/preprocessor.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'scaler': scaler,\n",
    "        'label_encoder': label_encoder,\n",
    "        'feature_names': features,\n",
    "        'num_classes': num_classes,\n",
    "        'sequence_length': SEQUENCE_LENGTH\n",
    "    }, f)\n",
    "\n",
    "# M√©tadonn√©es\n",
    "metadata = {\n",
    "    'n_train': len(X_train),\n",
    "    'n_val': len(X_val),\n",
    "    'n_test': len(X_test),\n",
    "    'n_features': X_train.shape[2],\n",
    "    'sequence_length': SEQUENCE_LENGTH,\n",
    "    'num_classes': num_classes,\n",
    "    'classes': list(label_encoder.classes_)\n",
    "}\n",
    "\n",
    "with open(f'{PROCESSED_DATA_DIR}/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Donn√©es sauvegard√©es dans: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"\\nüìÑ Fichiers cr√©√©s:\")\n",
    "for f in os.listdir(PROCESSED_DATA_DIR):\n",
    "    size = os.path.getsize(os.path.join(PROCESSED_DATA_DIR, f)) / (1024*1024)\n",
    "    print(f\"   - {f}: {size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà R√©sum√© du pr√©traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"R√âSUM√â\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "üìä Dataset:\n",
    "   - Flux totaux: {len(df_clean):,}\n",
    "   - S√©quences cr√©√©es: {len(X_seq):,}\n",
    "   - Features: {X_train.shape[2]}\n",
    "   - Longueur s√©quence: {SEQUENCE_LENGTH}\n",
    "   - Classes: {num_classes}\n",
    "\n",
    "üìÅ Fichiers sauvegard√©s:\n",
    "   - X_train.npy, X_val.npy, X_test.npy\n",
    "   - y_train.npy, y_val.npy, y_test.npy\n",
    "   - preprocessor.pkl (scaler + label_encoder)\n",
    "   - metadata.json\n",
    "\n",
    "‚û°Ô∏è Prochaine √©tape: Ex√©cuter 02_LSTM_Training.ipynb\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
